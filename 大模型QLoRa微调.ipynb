{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad546fcc-ab06-406d-8be3-e9edc488964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 12:49:39,764] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0613 12:49:41.126000 2192 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 12:49:41,616] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training,LoraConfig,get_peft_model,TaskType\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,TrainingArguments,Trainer,default_data_collator,BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb38da-f0ab-4c77-9789-3ae966c0e754",
   "metadata": {},
   "source": [
    "### QLoRa配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a756cb70-3930-4d36-a14c-d8236e6c22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "_compute_dtype_map = {\n",
    "    'fp32': torch.float32,\n",
    "    'fp16': torch.float16,\n",
    "    'bf16': torch.bfloat16\n",
    "}\n",
    "\n",
    "# QLoRA 量化配置\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                              bnb_4bit_quant_type='nf4',\n",
    "                              bnb_4bit_use_double_quant=True,\n",
    "                              bnb_4bit_compute_dtype=_compute_dtype_map['fp32'],\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c07780-6823-41a0-b188-6d82e4f37b0b",
   "metadata": {},
   "source": [
    "### 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f18d7b6-e5ca-4f18-adcc-c10ebf58f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"Qwen/Qwen3-0.6B\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name,device_map='auto',torch_dtype=\"auto\",quantization_config=q_config)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98b6008-4350-4b52-943b-de29b7013539",
   "metadata": {},
   "outputs": [],
   "source": [
    "kbit_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db597ea-6722-4023-8b34-afd8dd10e18a",
   "metadata": {},
   "source": [
    "### LoRa配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e817976d-1a5b-424c-b823-f0a58ba18a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,  # LoRA矩阵的秩\n",
    "    lora_alpha=32,  # LoRA alpha参数\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # 要应用LoRA的模块\n",
    "    lora_dropout=0.05,  # Dropout概率\n",
    "    bias=\"none\",  # 是否训练偏置\n",
    "    task_type=\"CAUSAL_LM\",  # 任务类型\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffa84e7-c530-4938-9644-0b3828c89d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_model = get_peft_model(kbit_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9877371-0673-4a4a-8db2-7b5b49c6f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,146,880 || all params: 597,196,800 || trainable%: 0.1920\n"
     ]
    }
   ],
   "source": [
    "qlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6431501-a72f-4c77-8334-36756ba1a7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109032c7-38b8-480e-9470-724ddb887676",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80fc5eeb-5a36-42e7-bc9d-63fdc493ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files={\n",
    "    'train': '../数据集/data/train.json',\n",
    "    'validation': '../数据集/data/eval.json'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d359c356-519f-42bf-a614-166c831b2d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029b036-eaaf-4143-b885-bac9e1486945",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d15b761-2db8-4ec9-b778-c411f3400d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fun(example):\n",
    "    question=[]\n",
    "    answer=[]\n",
    "    for i in example['conversations']:\n",
    "        for j in i:\n",
    "            if j['from']=='human':\n",
    "                question.append(j['value'])\n",
    "            elif j['from']=='gpt':\n",
    "                answer.append(j['value'])\n",
    "    return {'question':question,'answer':answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2db0bc-0598-4f1f-8745-7024b67d6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data=dataset.map(process_fun,batched=True,remove_columns=dataset['train'].column_names) # 提取问题和答案数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985780d3-1ddf-43d4-aeaa-6043758deac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 606\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 52\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ffef456-4caf-4d34-baf0-bbbb3b97534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '请问室温超导技术的难点在哪里,人类在什么时候可以实现室温超导?',\n",
       " 'answer': '室温超导技术的难点在于目前尚未发现或创造出能在室温下保持超导性的材料。常见的超导体需要在极低温（如液氦温度，约-269°C）下才能表现出零电阻和完全抗磁性的超导特性。尽管科学家们在高温超导材料研究方面取得了进展，目前已知的高温超导体在实现超导状态时所需的温度仍远远低于室温。\\n\\n实现室温超导的难点主要有以下几个方面：\\n\\n1. 材料问题：高温超导体多为陶瓷材料，这类材料往往脆弱难以制成长导线，并且许多超导材料在高温、高压条件下稳定性较差。\\n\\n2. 理论挑战：超导机制通常基于量子物理理论。对于传统的低温超导体，BCS理论（巴丁-库珀-施里弗理论）能够很好地描述其超导机制。但对于高温超导体，至今还没有一个普遍认可且完整的理论框架。\\n\\n3. 技术限制：即使找到合适的材料或者理论模型，通过现有的技术制备高质量的样品也是非常困难的。要保持超导状态需要的一系列条件如纯度、晶格结构的完整性等，在室温下实现和维持更加困难。\\n\\n4. 经济和实用性：即便科学家们能发现或制造出在室温下工作的超导材料，但其制造成本、可持续性和实际应用的有效性都是可能的挑战。\\n\\n至于人类能在什么时候实现室温超导，目前无法给出一个确切的时间表。室温超导是物理学中的一个激动人心但又极具挑战性的前沿议题，其研究进展依赖于新材料的发现、理论物理的突破以及实验技术的进步。可以说，室温超导的实现还处于科学探索的阶段，是未来技术发展具有重要潜力的领域之一。'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8b16068-5be5-4bca-86b8-841750d76770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fun(examples):\n",
    "    # 构建完整的指令格式（问：{问题}\\n答：{答案}）\n",
    "    instructions = []\n",
    "    for q, a in zip(examples['question'], examples['answer']):\n",
    "        instruction = f\"问：{q}\\n答：{a}\"\n",
    "        instructions.append(instruction)\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        instructions,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    labels = encoded[\"input_ids\"].clone()\n",
    "    \n",
    "    # 定位\"答：\"的位置，标记需要预测的部分\n",
    "    answer_start_token = tokenizer.encode(\"答：\", add_special_tokens=False)[0]\n",
    "    \n",
    "    # 遍历批次中的每个样本\n",
    "    for i in range(len(labels)):\n",
    "        # 找到每个样本中\"答：\"的第一个token位置\n",
    "        answer_positions = (labels[i] == answer_start_token).nonzero(as_tuple=True)[0]\n",
    "        if len(answer_positions) > 0:\n",
    "            # 只取第一个\"答：\"的位置\n",
    "            first_answer_pos = answer_positions[0]\n",
    "            # 将\"答：\"之前的token标记为-100（忽略计算损失）\n",
    "            labels[i, :first_answer_pos] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ddd82f-96c6-440e-b325-56ae70d5ff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b568603598664caa9f9be0dcffd4abc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9733503db436d8d24ffa821909e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = process_data.map(\n",
    "    tokenizer_fun,\n",
    "    batched=True,\n",
    "    remove_columns=process_data['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f87236f-1f9a-4b6d-87a0-2def42658f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 606\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9388b5d-b529-4254-8e0b-c4463f87b917",
   "metadata": {},
   "source": [
    "### 训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad09c0c-6f19-426c-bb74-a5937ce3a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Qlora_train_qwen_0.6B_model\",\n",
    "    logging_steps=100,\n",
    "    logging_dir='./runs',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    gradient_accumulation_steps=4,  # 如果GPU内存有限\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2afcab-d377-4718-86f2-6b6a743b0443",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b522e06f-e260-44d6-8b50-fd042896bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(\n",
    "    model=qlora_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc233cd3-b630-4e86-a9e5-1d92cb708f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "D:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 24:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.917908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.967466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.604531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-0.6B/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002642CD0D040>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 31dd60a0-2466-4a3b-8c90-48d276cc40a0)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-0.6B.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-0.6B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Anaconda\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-0.6B/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002642C7E9520>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 0601f4cf-81c6-49e3-9f1c-f910d0a23ae9)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-0.6B.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-0.6B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Anaconda\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 4f6adac0-e118-4edd-9bc8-9b25561eefde)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-0.6B.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-0.6B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57, training_loss=5.8237561677631575, metrics={'train_runtime': 1479.0308, 'train_samples_per_second': 1.229, 'train_steps_per_second': 0.039, 'total_flos': 2466370138669056.0, 'train_loss': 5.8237561677631575, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33789570-c6f2-4426-832b-7dceff3787c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
