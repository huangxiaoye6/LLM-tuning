{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd022c70-ec17-41d8-a6b9-60e0ab1e989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cae6dc-f776-41a2-8327-16f441cadb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 21:01:03,196] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0613 21:01:04.793000 7824 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 21:01:05,376] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    }
   ],
   "source": [
    "new_model=AutoModelForCausalLM.from_pretrained(\n",
    "    \"./train_qwen\",\n",
    "    device_map='auto',\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacc9e10-58a2-406f-a992-b28ae2170279",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('Qwen/Qwen3-0.6B')\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51749b4c-3444-4f21-9c78-6b50a7efac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户: 以西游记的孙悟空的风格来生成回复。为什么星球基本都是圆的？\n",
      "模型: <think>\n",
      "嗯，用户让我用《西游记》中孙悟空的回答去解释“为什么星球基本上都是圆的”。首先，我需要理解这个提问背后的意图和可能的背景信息。通常来说，宇宙中的天体都是球形或者接近球状的结构，这与物理学上的经典理论一致——即牛顿万有引力定律下物体围绕中心点运动时形成的圆形轨道。\n",
      "\n",
      "不过，作为孙悟空，在面对这种科学原理时可能会有一些幽默或超自然的答案。他擅长于用夸张、狡猾的语言表达自己的想法，并且喜欢将复杂概念简化成有趣的小故事或寓言形式。\n",
      "\n",
      "接下来要考虑的是如何让回答既符合角色设定又保持逻辑性。比如可以提到银河系内各种奇特的现象，或是通过一些神奇的方法（如使用金箍棒等工具）来进行简单的天文观察，甚至加入一些神话元素，比如提到星斗大王之类的传说人物，使整个过程显得更加生动而富有想象力。\n",
      "\n",
      "同时还要注意语气要体现出孙悟空那种机智而不失威严的性格特征。可以通过描述那些看似普通的日常现象，却在背后隐藏着深奥的道理，来展示他的思维深度以及对世界的独特见解。\n",
      "\n",
      "最后，确保整个回应流畅连贯，没有生硬之处，并充分表达了用户所期待的信息。\n",
      "</think>\n",
      "\n",
      "啊，这位大师大人！你问得倒妙呢，要说说我们这些小虫子为何能飞过那片浩瀚无垠的大海，绕道银河之间走一回？\n",
      "\n",
      "其实嘛，俺老孙虽是神仙，但见识有限，只知些奇妙的事物罢了。就像你们这般看世界，总是在寻找答案，却又不晓得其中道理！\n",
      "\n",
      "若论星辰之行，乃是因宇宙本就遵循着某种规律运行：所有事物都在追寻一个共同的方向，就如同我们每一步都向着同一个目标前进一样。即使有时会遇到阻碍，也别怪啦！\n",
      "\n",
      "再说到行星形状的话……哦，原来是这样，咱们可得像孙悟空那样，先从头到尾地研究清楚每一个细节，才能真正弄明白事情的真谛呀！\n"
     ]
    }
   ],
   "source": [
    "def chat_with_model(model, tokenizer, user_input, max_length=512):\n",
    "    # 构建对话格式\n",
    "    prompt = f\"<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    # 编码输入\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # 关键修复：将输入张量移动到与模型相同的设备\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 生成回复\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # 解码输出\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # 提取助手回复部分\n",
    "    assistant_start = \"<|im_start|>assistant\\n\"\n",
    "    assistant_end = \"<|im_end|>\"\n",
    "    if assistant_start in response and assistant_end in response:\n",
    "        response = response.split(assistant_start)[1].split(assistant_end)[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 测试对话\n",
    "user_input = \"以西游记的孙悟空的风格来生成回复。为什么星球基本都是圆的？\"\n",
    "response = chat_with_model(new_model, tokenizer, user_input)\n",
    "print(f\"用户: {user_input}\")\n",
    "print(f\"模型: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000c21d-344d-4375-a6cb-39f809a1f779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
