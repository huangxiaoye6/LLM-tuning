{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GRPO强化学习",
   "id": "3c46f6acd8b73bfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:11:31.487833Z",
     "start_time": "2026-02-24T11:11:02.119386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOTrainer,GRPOConfig\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "d05515c33c79ed3a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1、加载数据集",
   "id": "aa55bcf35e79fb7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:16:41.312017Z",
     "start_time": "2026-02-23T09:16:35.167748Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"meta-math/GSM8K_zh\", split=\"train[:1000]\").select_columns(column_names=['question_zh','answer_zh','answer_only'])",
   "id": "56c6085a354658c8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:16:41.396629Z",
     "start_time": "2026-02-23T09:16:41.380689Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "f24c8c885a704fb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question_zh', 'answer_zh', 'answer_only'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2、数据预处理",
   "id": "ce7b6bde65acab66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:16:41.472971Z",
     "start_time": "2026-02-23T09:16:41.458275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def covert_data(example):\n",
    "    return {'prompt':[{'role':'user','content':example['question_zh']}],'solution':example['answer_only']}"
   ],
   "id": "5a5492016b850948",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:16:41.525778Z",
     "start_time": "2026-02-23T09:16:41.505889Z"
    }
   },
   "cell_type": "code",
   "source": "dataset=dataset.map(covert_data,remove_columns=dataset.column_names)",
   "id": "539c993b7eb6cab7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:16:41.541523Z",
     "start_time": "2026-02-23T09:16:41.525778Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "7943bbd9cbeccb5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3、定义奖励函数",
   "id": "f10d217f65d33747"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:21:34.031911Z",
     "start_time": "2026-02-23T09:21:34.013173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def accuracy_reward(**kwargs):\n",
    "    solution = str(kwargs[\"solution\"]).strip()\n",
    "    completions = kwargs[\"completions\"]  # 注意这里\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for comp in completions:\n",
    "        output = comp.get(\"content\", \"\")\n",
    "        numbers = re.findall(r\"\\d+\", output)\n",
    "        if not numbers:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        pred = numbers[-1].strip()\n",
    "        rewards.append(1.0 if pred == solution else 0.0)\n",
    "\n",
    "    return rewards"
   ],
   "id": "848eac6b1651fd95",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3、配置GRPOConfig",
   "id": "492d84da033b2c1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:21:35.726315Z",
     "start_time": "2026-02-23T09:21:35.635653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args=GRPOConfig(\n",
    "    output_dir='../../models/grpo_model',\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_generations=2,\n",
    "    num_train_epochs=1,\n",
    ")"
   ],
   "id": "a3ae0a9a29efc790",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:21:45.642075Z",
     "start_time": "2026-02-23T09:21:36.181538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    reward_funcs=accuracy_reward,\n",
    "    train_dataset=dataset,\n",
    ")"
   ],
   "id": "4b6f6b97d20c061b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 311/311 [00:00<00:00, 350.56it/s, Materializing param=model.norm.weight]                              \n",
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:23:32.691756Z",
     "start_time": "2026-02-23T09:21:45.660373Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "62798edee8b0ed58",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\transformers\\trainer.py:2170\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2168\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2169\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2171\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2175\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\transformers\\trainer.py:2537\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2530\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2531\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2532\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2533\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2534\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2535\u001B[0m )\n\u001B[0;32m   2536\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2537\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2540\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2541\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2542\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2543\u001B[0m ):\n\u001B[0;32m   2544\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2545\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\trl\\trainer\\grpo_trainer.py:1038\u001B[0m, in \u001B[0;36mGRPOTrainer.training_step\u001B[1;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, inputs, num_items_in_batch):\n\u001B[0;32m   1037\u001B[0m     time_before \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m-> 1038\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1039\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1040\u001B[0m     time_after \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\transformers\\trainer.py:3804\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mtrain):\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m-> 3804\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[0;32m   3806\u001B[0m     loss_mb \u001B[38;5;241m=\u001B[39m smp_forward_backward(model, inputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps)\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\trl\\extras\\profiling.py:202\u001B[0m, in \u001B[0;36mprofiling_decorator.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m profiling_context(\u001B[38;5;28mself\u001B[39m, func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m):\n\u001B[1;32m--> 202\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    203\u001B[0m \u001B[38;5;66;03m# For non-Trainer objects (e.g., VLLMGeneration), use ProfilingContext directly\u001B[39;00m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccelerator\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\trl\\trainer\\grpo_trainer.py:1067\u001B[0m, in \u001B[0;36mGRPOTrainer._prepare_inputs\u001B[1;34m(self, generation_batch)\u001B[0m\n\u001B[0;32m   1064\u001B[0m generate_every \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msteps_per_generation \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_iterations\n\u001B[0;32m   1065\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step \u001B[38;5;241m%\u001B[39m generate_every \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffered_inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001B[39;00m\n\u001B[1;32m-> 1067\u001B[0m     generation_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_and_score_completions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1068\u001B[0m     generation_batch \u001B[38;5;241m=\u001B[39m split_pixel_values_by_grid(generation_batch)\n\u001B[0;32m   1069\u001B[0m     generation_batch \u001B[38;5;241m=\u001B[39m shuffle_sequence_dict(generation_batch)\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\trl\\trainer\\grpo_trainer.py:1731\u001B[0m, in \u001B[0;36mGRPOTrainer._generate_and_score_completions\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   1726\u001B[0m                 inp[key] \u001B[38;5;241m=\u001B[39m values\n\u001B[0;32m   1728\u001B[0m \u001B[38;5;66;03m# Calculate rewards for each reward function. rewards_per_func aggregates rewards across all processes. This is\u001B[39;00m\n\u001B[0;32m   1729\u001B[0m \u001B[38;5;66;03m# important because rewards will be normalized per group, and completions are distributed. We will later slice\u001B[39;00m\n\u001B[0;32m   1730\u001B[0m \u001B[38;5;66;03m# rewards_per_func to extract each process's subset.\u001B[39;00m\n\u001B[1;32m-> 1731\u001B[0m rewards_per_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calculate_rewards\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompletions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompletion_ids_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1732\u001B[0m num_generations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_generations \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_generations_eval\n\u001B[0;32m   1734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmulti_objective_aggregation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum_then_normalize\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1735\u001B[0m     \u001B[38;5;66;03m# Apply weights to each reward function's output and sum\u001B[39;00m\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\trl\\extras\\profiling.py:202\u001B[0m, in \u001B[0;36mprofiling_decorator.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m profiling_context(\u001B[38;5;28mself\u001B[39m, func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m):\n\u001B[1;32m--> 202\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    203\u001B[0m \u001B[38;5;66;03m# For non-Trainer objects (e.g., VLLMGeneration), use ProfilingContext directly\u001B[39;00m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccelerator\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mE:\\web\\LLM-tuning\\.venv\\lib\\site-packages\\trl\\trainer\\grpo_trainer.py:1117\u001B[0m, in \u001B[0;36mGRPOTrainer._calculate_rewards\u001B[1;34m(self, inputs, prompts, completions, completion_ids_list)\u001B[0m\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1115\u001B[0m     \u001B[38;5;66;03m# Run synchronous reward function\u001B[39;00m\n\u001B[0;32m   1116\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m profiling_context(\u001B[38;5;28mself\u001B[39m, reward_func_name):\n\u001B[1;32m-> 1117\u001B[0m         output_reward_func \u001B[38;5;241m=\u001B[39m reward_func(\n\u001B[0;32m   1118\u001B[0m             prompts\u001B[38;5;241m=\u001B[39mprompts, completions\u001B[38;5;241m=\u001B[39mcompletions, completion_ids\u001B[38;5;241m=\u001B[39mcompletion_ids_list, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mreward_kwargs\n\u001B[0;32m   1119\u001B[0m         )\n\u001B[0;32m   1120\u001B[0m         \u001B[38;5;66;03m# Convert None values to NaN\u001B[39;00m\n\u001B[0;32m   1121\u001B[0m         output_reward_func \u001B[38;5;241m=\u001B[39m [reward \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mnan \u001B[38;5;28;01mfor\u001B[39;00m reward \u001B[38;5;129;01min\u001B[39;00m output_reward_func]\n",
      "Cell \u001B[1;32mIn[11], line 9\u001B[0m, in \u001B[0;36maccuracy_reward\u001B[1;34m(**kwargs)\u001B[0m\n\u001B[0;32m      6\u001B[0m rewards \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m comp \u001B[38;5;129;01min\u001B[39;00m completions:\n\u001B[1;32m----> 9\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mcomp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m     numbers \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+\u001B[39m\u001B[38;5;124m\"\u001B[39m, output)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m numbers:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "663331d45446b116"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e0dd4e8d37e6093"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
